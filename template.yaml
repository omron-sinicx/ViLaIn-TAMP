theme: default # default || dark
organization: OMRON SINIC X
twitter: '@omron_sinicx'
title: 'Grounded Vision-Language Interpreter for Integrated Task and Motion Planning'
conference: In Review
resources:
  video:
#   paper:
#   poster:
#   code:
#   blog:
#   demo:
#   huggingface:
description: While recent advances in vision-language models (VLMs) have accelerated the development of language-guided robot planners, their black-box nature often lacks safety guarantees and interpretability crucial for real-world deployment. This paper proposes ViLaIn-TAMP, a hybrid planning framework for enabling verifiable, interpretable, and autonomous robot behaviors.
#image: https://omron-sinicx.github.io/${your-repository-name}/teaser.png
url: https://omron-sinicx.github.io/${your-repository-name}
speakerdeck:
authors:
  - name: Jeremy Siburian*
    affiliation: [1]
    url: https://jsiburian.github.io/
    position: equal contribution
  - name: Keisuke Shirai*
    affiliation: [2]
    url: https://kskshr.github.io
    position: equal contribution
  - name: Cristian C. Beltran-Hernandez*
    affiliation: [2]
    url: https://cristianbehe.me
    position: equal contribution
  - name: Masashi Hamaya
    affiliation: [2]
    position: Senior Researcher
    url: https://sites.google.com/view/masashihamaya/home
  - name: Michael GÃ¶rner
    affiliation: [3]
    position: Professor
    url: https://tams.informatik.uni-hamburg.de/people/goerner/
  - name: Atsushi Hashimoto
    affiliation: [2]
    position: Senior Researcher
    url: https://atsushihashimoto.github.io/cv
contact_ids: ['github', 'omron', 3] #=> github issues, contact@sinicx.com, 2nd author
affiliations:
  - The University of Tokyo
  - OMRON SINIC X Corporation
  - University of Hamburg
meta:
  - '* Equal contribution'
# bibtex: >
#   TBW

header:
  bg_curve: sinic_curve.png

<<<<<<< HEAD
#teaser: teaser.png
=======
teaser: |
  <video
    src="Slicing-Problem1-Optimized-x3.mp4"
    autoplay
    loop
    muted
    playsinline
    style="max-width: 100%; height: auto;"
  />
>>>>>>> c9042da (add video)
overview: |
  <video
    src="Slicing-Problem1-Optimized-x3.mp4"
    autoplay
    loop
    muted
    playsinline
    style="max-width: 100%; height: auto;"
  />

  ViLaIn-TAMP is a hybrid planning framework that bridges the gap between vision-language models and classical symbolic planners. While recent advances in vision-language models (VLMs) have accelerated the development of language-guided robot planners, their black-box nature often lacks safety guarantees and interpretability crucial for real-world deployment. Conversely, classical symbolic planners offer rigorous safety verification but require significant expert knowledge for setup.

  Our framework comprises three main components: (1) a <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">Vision-Language Interpreter (ViLaIn)</b> for translating multimodal inputs into PDDL problems, (2) a <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;"><i>sequence-before-satisfy</i></b> <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">TAMP module</b> for finding symbolically complete, collision-free action plans, and (3) a <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">corrective planning module</b> for refining outputs based on grounded failure feedback.

body:
  - title: Framework Overview
    text: |
      The ViLaIn-TAMP framework consists of three major components: (1) a <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">Vision-Language Interpreter (ViLaIn)</b> for translating multimodal inputs into PDDL problems, (2) a <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;"><i>sequence-before-satisfy</i></b> <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">TAMP module</b> for finding symbolically complete, collision-free action plans, and (3) a <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">corrective planning module</b> for refining outputs based on grounded failure feedback.

      <img src="teaser.png" alt="ViLaIn-TAMP Framework" class="uk-align-center uk-responsive-width" />
      <span class="uk-text-meta">ViLaIn-TAMP Framework Architecture</span>

  - title: Manipulation Tasks
    text: |
      We evaluate ViLaIn-TAMP on five manipulation tasks in a cooking domain:

      - <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">Pick and Place</b>: moves a target object to a desired location.
      - <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">Pick Obstacles Dual Arm</b>: moves a target object to a desired location while removing another object which occupies the target location with dual arms.
      - <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">Pick Obstacles Single Arm</b>: the same task as <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">Pick Obstacles Dual Arm</b> but with a single arm.
      - <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">Slice Food</b>: slices a food (e.g., vegetable or fruit) using a tool (e.g., knife).
      - <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">Slice and Serve</b>: slices a food using a tool and serves the slices in a desired location (e.g., bowl or plate).

  - title: Experimental Results
    text: |
      We consider four model configurations to evaluate ViLaIn-TAMP:

      - <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">ViLaIn-TAMP-CP</b>: A ViLaIn-TAMP *with* corrective planning (CP).
      - <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">ViLaIn-TAMP-No-CP</b>: ViLaIn-TAMP *without* CP.
      - <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">Baseline-CP</b>: An approach that uses VLMs to directly generate actions plans *with* CP.
      - <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">Baseline-No-CP</b>: The baseline approach *without* CP.

      The results show that (1) ViLaIn-TAMP outperforms the baseline approach by a large margin, and (2) CP constantly improves success rates. The proposed closed-loop corrective architecture exhibits a more than <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">30% higher mean success rate</b> for ViLaIn-TAMP compared to without corrective planning.

      <img src="main_results.png" alt="Experimental Results" class="uk-align-center uk-responsive-width" />
      <span class="uk-text-meta">Main experimental results showing success rates across different tasks and configurations</span>

  - title: Key Contributions
    text: |
      Our work makes several key contributions to the field of robot planning:

      1. <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">Hybrid Planning Framework</b>: We bridge the gap between VLMs and classical symbolic planners, combining the strengths of both approaches.

      2. <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">Interpretable and Verifiable</b>: Unlike black-box VLM planners, our approach provides interpretable problem specifications and safety guarantees through symbolic reasoning.

      3. <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">Corrective Planning</b>: Our framework includes a corrective planning module that learns from failures and improves performance through iterative refinement.

      4. <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">Domain-Agnostic</b>: The ViLaIn component works with off-the-shelf VLMs without requiring additional domain-specific training.

      5. <b style="font-weight: 900; text-shadow: 0.5px 0 0 currentColor;">Comprehensive Evaluation</b>: We demonstrate the effectiveness of our approach across multiple manipulation tasks in a cooking domain.

# projects: []
