theme: default # default || dark
organization: OMRON SINIC X
twitter: '@omron_sinicx'
title: 'Grounded Vision-Language Interpreter for Integrated Task and Motion Planning'
conference: In Review
resources:
  video:
#   paper:
#   poster:
#   code:
#   blog:
#   demo:
#   huggingface:
description: While recent advances in vision-language models (VLMs) have accelerated the development of language-guided robot planners, their black-box nature often lacks safety guarantees and interpretability crucial for real-world deployment. This paper proposes ViLaIn-TAMP, a hybrid planning framework for enabling verifiable, interpretable, and autonomous robot behaviors.
#image: https://omron-sinicx.github.io/${your-repository-name}/teaser.png
url: https://omron-sinicx.github.io/${your-repository-name}
speakerdeck:
authors:
  - name: Jeremy Siburian*
    affiliation: [1]
    url: https://jsiburian.github.io/
    position: equal contribution
  - name: Keisuke Shirai*
    affiliation: [2]
    url: https://kskshr.github.io
    position: equal contribution
  - name: Cristian C. Beltran-Hernandez*
    affiliation: [2]
    url: https://cristianbehe.me
    position: equal contribution
  - name: Masashi Hamaya
    affiliation: [2]
    position: Senior Researcher
    url: https://sites.google.com/view/masashihamaya/home
  - name: Michael GÃ¶rner
    affiliation: [3]
    position: Professor
    url: https://tams.informatik.uni-hamburg.de/people/goerner/
  - name: Atsushi Hashimoto
    affiliation: [2]
    position: Senior Researcher
    url: https://atsushihashimoto.github.io/cv
contact_ids: ['github', 'omron', 3] #=> github issues, contact@sinicx.com, 2nd author
affiliations:
  - The University of Tokyo
  - OMRON SINIC X Corporation
  - University of Hamburg
meta:
  - '* Equal contribution'
# bibtex: >
#   TBW

header:
  bg_curve: sinic_curve.png

#teaser: teaser.png
overview: |
  ViLaIn-TAMP is a hybrid planning framework that bridges the gap between vision-language models and classical symbolic planners. While recent advances in vision-language models (VLMs) have accelerated the development of language-guided robot planners, their black-box nature often lacks safety guarantees and interpretability crucial for real-world deployment. Conversely, classical symbolic planners offer rigorous safety verification but require significant expert knowledge for setup.

  Our framework comprises three main components: (1) **ViLaIn** (Vision-Language Interpreter) - converts multimodal inputs into structured problem specifications using off-the-shelf VLMs without additional domain-specific training, (2) a modular **Task and Motion Planning (TAMP)** system that grounds these specifications in actionable trajectory sequences through symbolic and geometric constraint reasoning, and (3) a **corrective planning module** which receives concrete feedback on failed solution attempts and can feed adapted logic and geometric feasibility constraints back to ViLaIn to improve and further refine the specification.

body:
  - title: Framework Overview
    text: |
      The ViLaIn-TAMP framework consists of three major components: (1) a Vision-Language Interpreter (ViLaIn) for translating multimodal inputs into PDDL problems, (2) a sequence-before-safety TAMP module for finding symbolically complete, collision-free action plans, and (3) a corrective planning module for refining outputs based on grounded failure feedback.

      <img src="teaser.png" alt="ViLaIn-TAMP Framework" class="uk-align-center uk-responsive-width" />
      <span class="uk-text-meta">ViLaIn-TAMP Framework Architecture</span>

  - title: Manipulation Tasks
    text: |
      We evaluate ViLaIn-TAMP on five manipulation tasks in a cooking domain:

      - **Pick and Place**: moves a target object to a desired location.
      - **Pick Obstacles Dual Arm**: moves a target object to a desired location while removing another object which occupies the target location with dual arms.
      - **Pick Obstacles Single Arm**: the same task as Pick Obstacles Dual Arm but with a single arm.
      - **Slice Food**: slices a food (e.g., vegetable or fruit) using a tool (e.g., knife).
      - **Slice and Serve**: slices a food using a tool and serves the slices in a desired location (e.g., bowl or plate).

  - title: Experimental Results
    text: |
      We consider four model configurations to evaluate ViLaIn-TAMP:

      - **ViLaIn-TAMP-CP**: A ViLaIn-TAMP *with* corrective planning (CP).
      - **ViLaIn-TAMP-No-CP**: ViLaIn-TAMP *without* CP.
      - **Baseline-CP**: An approach that uses VLMs to directly generate actions plans *with* CP.
      - **Baseline-No-CP**: The baseline approach *without* CP.

      The results show that (1) ViLaIn-TAMP outperforms the baseline approach by a large margin, and (2) CP constantly improves success rates. The proposed closed-loop corrective architecture exhibits a more than **30% higher mean success rate** for ViLaIn-TAMP compared to without corrective planning.

      <img src="main_results.png" alt="Experimental Results" class="uk-align-center uk-responsive-width" />
      <span class="uk-text-meta">Main experimental results showing success rates across different tasks and configurations</span>

  - title: Key Contributions
    text: |
      Our work makes several key contributions to the field of robot planning:

      1. **Hybrid Planning Framework**: We bridge the gap between VLMs and classical symbolic planners, combining the strengths of both approaches.

      2. **Interpretable and Verifiable**: Unlike black-box VLM planners, our approach provides interpretable problem specifications and safety guarantees through symbolic reasoning.

      3. **Corrective Planning**: Our framework includes a corrective planning module that learns from failures and improves performance through iterative refinement.

      4. **Domain-Agnostic**: The ViLaIn component works with off-the-shelf VLMs without requiring additional domain-specific training.

      5. **Comprehensive Evaluation**: We demonstrate the effectiveness of our approach across multiple manipulation tasks in a cooking domain.

# projects: []
