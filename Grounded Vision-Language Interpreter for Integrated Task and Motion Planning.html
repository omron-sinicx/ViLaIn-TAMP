<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>
      Grounded Vision-Language Interpreter for Integrated Task and Motion Planning
    </title>

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.7.0/font/bootstrap-icons.css" rel="stylesheet">
    <link rel="stylesheet" href="assets/css/main.css">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-7DWSR8EM02"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-7DWSR8EM02');
    </script>

    <meta property="og:url" content="https://jsiburian.github.io/vilain-tamp" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="
Grounded Vision-Language Interpreter for Integrated Task and Motion Planning
    " />
    <meta property="og:description" content="
While recent advances in vision-language models (VLMs) have accelerated the development of language-guided robot planners, their black-box nature often lacks safety guarantees and interpretability crucial for real-world deployment. Conversely, classical symbolic planners offer rigorous safety verification but require significant expert knowledge for setup. To bridge the current gap, this paper proposes ViLaIn-TAMP, a hybrid planning framework for enabling verifiable, interpretable, and autonomous robot behaviors. ViLaIn-TAMP comprises three main components: (1) ViLaIn (Vision-Language Interpreter) - A prior framework that converts multimodal inputs into structured problem specifications using off-the-shelf VLMs without additional domain-specific training, (2) a modular Task and Motion Planning (TAMP) system that grounds these specifications in actionable trajectory sequences through symbolic and geometric constraint reasoning and can utilize learning-based skills for key manipulation phases, and (3) a corrective planning module which receives concrete feedback on failed solution attempts from the motion and task planning components and can feed adapted logic and geometric feasibility constraints back to ViLaIn to improve and further refine the specification. We evaluate our framework on several challenging manipulation tasks in a cooking domain. We demonstrate that the proposed closed-loop corrective architecture exhibits a more than 30% higher mean success rate for ViLaIn-TAMP compared to without corrective planning.
    " />
    <!-- <meta property="og:image" content="https://jsiburian.github.io/vilain-tamp/assets/img/banner.png" /> -->

  </head>
  <body>
    <div class="container-fluid">
      <div class="row">
        <div class="col-lg-8 offset-lg-2 col-md-12">

          <div class="text-center">
            <h1 class="mt-5 publication-title"><b>Grounded Vision-Language Interpreter for Integrated Task and Motion Planning</b></h1>
            <ul class="list-inline mt-4">
              <li class="list-inline-item ml-4"><a href="https://jsiburian.github.io/" target="_blank">Jeremy Siburian</a><sup>1*</sup></li>
              <li class="list-inline-item ml-4"><a href="https://kskshr.github.io" target="_blank">Keisuke Shirai</a><sup>2*</sup></li>
              <li class="list-inline-item ml-4"><a href="https://cristianbehe.me" target="_blank">Cristian C. Beltran-Hernandez</a><sup>2*</sup></li>
            </ul>
            <ul>
              <li class="list-inline-item ml-4"><a href="https://sites.google.com/view/masashihamaya/home" target="_blank">Masashi Hamaya</a><sup>2</sup></li>
              <li class="list-inline-item ml-4"><a href="https://tams.informatik.uni-hamburg.de/people/goerner/" target="_blank">Michael GÃ¶rner</a><sup>3</sup></li>
              <li class="list-inline-item ml-4"><a href="https://atsushihashimoto.github.io/cv" target="_blank">Atsushi Hashimoto</a><sup>2</sup></li>

              <br>
              <li class="list-inline-item mt-2"><sup>1</sup>The University of Tokyo</li>
              <li class="list-inline-item mt-2"><sup>2</sup>OMRON SINIC X Corporation</li>
              <li class="list-inline-item mt-2"><sup>3</sup>University of Hamburg</li>
              <br>
              <li class="list-inline-item mt-2"><sup>*</sup>Equal contribution</li>
            </ul>

            <ul class="list-inline mt-4">
              <li class="list-inline-item">
                <a href="TBW" target="_blank">Paper</a>
              </li>
            </ul>

          </div>

          <div class="row mt-4">
            <div class="col-lg-10 offset-lg-1">
              <h4 align="center" class="page-block">Abstract</h4>
              <p>
While recent advances in vision-language models (VLMs) have accelerated the development of language-guided robot planners, their black-box nature often lacks safety guarantees and interpretability crucial for real-world deployment. Conversely, classical symbolic planners offer rigorous safety verification but require significant expert knowledge for setup. To bridge the current gap, this paper proposes ViLaIn-TAMP, a hybrid planning framework for enabling verifiable, interpretable, and autonomous robot behaviors. ViLaIn-TAMP comprises three main components: (1) ViLaIn (Vision-Language Interpreter) - A prior framework that converts multimodal inputs into structured problem specifications using off-the-shelf VLMs without additional domain-specific training, (2) a modular Task and Motion Planning (TAMP) system that grounds these specifications in actionable trajectory sequences through symbolic and geometric constraint reasoning and can utilize learning-based skills for key manipulation phases, and (3) a corrective planning module which receives concrete feedback on failed solution attempts from the motion and task planning components and can feed adapted logic and geometric feasibility constraints back to ViLaIn to improve and further refine the specification. We evaluate our framework on several challenging manipulation tasks in a cooking domain. We demonstrate that the proposed closed-loop corrective architecture exhibits a more than 30% higher mean success rate for ViLaIn-TAMP compared to without corrective planning.
              </p>
            </div>
          </div>
        
          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4 align="center" class="page-block">ViLaIn-TAMP Framework</h4>
            <p>The ViLaIn-TAMP framework consists of three major components: (1) a Vision-Language Interpreter (ViLaIn) for translating multimodal inputs into PDDL problems, (2) a sequence-before-safety TAMP module for finding symbolically complete, collision-free action plans, and (3) a corrective planning module for refining outputs based on grounded failure feedback.
            <div align="center" class="row mt-4">
              <div class="col-lg-12">
                <img src="assets/img/vilain-tamp_framework.png" class="img-fluid"/>
              </div>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4 align="center" class="page-block">Tasks</h4>
            <p>We evaluate ViLaIn-TAMP on five manipulation tasks in a cooking domain.
            <ul>
                <li><b>Pick and Place</b> moves a target object to a desired location.</li>
                <li><b>Pick Obstacles Dual Arm</b> moves a target object to a desired location while removing another object which occupies the target location with dual rams.</li>
                <li><b>Pick Obstacles Single Arm</b> is the same task as <b>Pick Obstacles Dual Arm</b> but with a single arm.</li>
                <li><b>Slice Food</b> slices a food (e.g., vegetable or fruit) using a tool (e.g., knife).</li>
                <li><b>Slice and Serve</b> slices a food using a tool and serves the slices in a desired location (e.g., bowl or plate).</li>
            </ul>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4 align="center" class="page-block">Experimental Evaluation</h4>
            <p>We consider four model configurations to evaluate ViLaIn-TAMP.
            <ul>
                <li><b>ViLaIn-TAMP-CP</b>: A ViLaIn-TAMP <u>with</u> corrective planning (CP).</li>
                <li><b>ViLaIn-TAMP-No-CP</b>: ViLaIn-TAMP <u>without</u> CP.</li>
                <li><b>Baseline-CP</b>: An approach that uses VLMs to directly generate actions plans <u>with</u> CP.</li>
                <li><b>Baseline-No-CP</b>: The baseline approach <u>without</u> CP.</li>
            </ul>
            The results show that (1) ViLaIn-TAMP outperforms the baseline approach by a large marin, and (2) CP constantly improves success rates.
            <div align="center" class="row mt-4">
              <div class="col-lg-12">
                <img src="assets/img/main_results.png" class="img-fluid"/>
              </div>
            </div>
          </div>


          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4 align="center" class="page-block">Citation</h4>
            <pre>
TBW
            </pre>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4 align="center" class="page-block">Acknowledgements</h4>
            <p>
This work was supported by JSPS KAKENHI Grant Numbers 25K21274.
            </p>
          </div>
          <br>

        </div>
      </div>
    </div>
  </body>
</html>
